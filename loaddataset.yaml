name: Load Medical Diagnosis CSV
description: Downloads a Medical Diagnosis CSV file from a CDN, splits it into train/test datasets, and generates dataset metadata.

inputs:
  - {name: cdn_url, type: String, description: 'CDN URL to download CSV file'}
  - {name: target_column, type: String, description: 'Target column name for classification (e.g., has_disease)'}
  - {name: train_split, type: Float, description: 'Train split ratio (default 0.75)'}
  - {name: shuffle_seed, type: Integer, description: 'Random seed for shuffling'}

outputs:
  - {name: train_data, type: Dataset}
  - {name: test_data, type: Dataset}
  - {name: dataset_info, type: DatasetInfo}

implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        pip install pandas numpy requests scikit-learn >/dev/null 2>&1
        python -c "
        import os
        import sys
        import io
        import pandas as pd
        import numpy as np
        import requests
        import pickle
        from urllib.parse import unquote
        from sklearn.model_selection import train_test_split

        cdn_url = sys.argv[1]
        target_column = sys.argv[2]
        train_split = float(sys.argv[3])
        shuffle_seed = int(sys.argv[4])
        train_data_path = sys.argv[5]
        test_data_path = sys.argv[6]
        dataset_info_path = sys.argv[7]

        print('Medical Diagnosis CSV Loader')
        print(f'CDN URL: {cdn_url}')
        print(f'Target column: {target_column}')
        print(f'Train split: {train_split}')
        print(f'Shuffle seed: {shuffle_seed}')

        # Validate train_split parameter
        if train_split > 1.0:
            print(f'Warning: train_split {train_split} is greater than 1.0, converting to ratio')
            train_split = train_split / 100.0
            print(f'Converted train_split to: {train_split}')

        decoded_url = unquote(cdn_url)
        try:
            print('Downloading CSV')
            response = requests.get(decoded_url, timeout=30)
            response.raise_for_status()
            df = pd.read_csv(io.BytesIO(response.content))
            print(f'Loaded CSV with shape: {df.shape}')
            print(f'Columns: {list(df.columns)}')
        except Exception as e:
            print(f'Failed to load dataset: {e}')
            raise

        # Check if target is binary for classification
        target_unique = df[target_column].nunique()
        target_distribution = df[target_column].value_counts()
        print(f'Target variable has {target_unique} unique values: {target_distribution.to_dict()}')

        if target_unique > 2:
            print('Warning: Target has more than 2 unique values - using binary classification mode')

        # Basic data validation
        print('Data Summary:')
        print(df.describe())
        print('Missing values:')
        print(df.isnull().sum())

        # Ensure train_split is between 0 and 1
        train_split = max(0.1, min(0.9, train_split))
        test_size = 1.0 - train_split
        
        print(f'Using train split: {train_split}, test size: {test_size}')

        # Stratified split for classification
        train_df, test_df = train_test_split(
            df, 
            test_size=test_size, 
            random_state=shuffle_seed,
            stratify=df[target_column]
        )

        print(f'Train shape: {train_df.shape}, Test shape: {test_df.shape}')
        print(f'Class distribution in train: {train_df[target_column].value_counts().to_dict()}')
        print(f'Class distribution in test: {test_df[target_column].value_counts().to_dict()}')

        feature_columns = [col for col in df.columns if col != target_column]
        
        # Identify feature types based on your dataset
        numeric_features = ['age', 'blood_pressure', 'cholesterol', 'bmi']
        categorical_features = ['genetic_marker']  # This is binary (0,1) but we'll treat as categorical
        
        dataset_info = {
            'total_samples': len(df),
            'train_samples': len(train_df),
            'test_samples': len(test_df),
            'target_column': target_column,
            'feature_columns': feature_columns,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'train_split_ratio': train_split,
            'shuffle_seed': shuffle_seed,
            'columns': list(df.columns),
            'dtypes': {col: str(df[col].dtype) for col in df.columns},
            'target_distribution': df[target_column].value_counts().to_dict(),
            'problem_type': 'classification',
            'dataset_name': 'medical_diagnosis'
        }

        # Ensure directories exist before saving
        os.makedirs(os.path.dirname(train_data_path) or '.', exist_ok=True)
        train_df.to_csv(train_data_path, index=False)

        os.makedirs(os.path.dirname(test_data_path) or '.', exist_ok=True)
        test_df.to_csv(test_data_path, index=False)

        os.makedirs(os.path.dirname(dataset_info_path) or '.', exist_ok=True)
        with open(dataset_info_path, 'wb') as f:
            pickle.dump(dataset_info, f)

        print('Dataset processing complete!')
        print(f'Train data saved at: {train_data_path}')
        print(f'Test data saved at: {test_data_path}')
        print(f'Dataset info keys: {list(dataset_info.keys())}')
        " "$0" "$1" "$2" "$3" "$4" "$5" "$6"
    args:
      - {inputValue: cdn_url}
      - {inputValue: target_column}
      - {inputValue: train_split}
      - {inputValue: shuffle_seed}
      - {outputPath: train_data}
      - {outputPath: test_data}
      - {outputPath: dataset_info}
